#1

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

class InvertedIndex:
    def __init__(self):
        self.index = {}

    def add_document(self, doc_id, content):
        words = word_tokenize(content.lower())
        stop_words = set(stopwords.words('english'))
        for word in set(words):
            if word.isalnum() and word not in stop_words:
                self.index.setdefault(word, []).append(doc_id)

    def search(self, query):
        return self.index.get(query.lower(), [])

# Example
docs = {
    1: "Inverted index is a data structure",
    2: "Example of inverted index construction",
    3: "Algorithm constructs an inverted index"
}

idx = InvertedIndex()
for id, text in docs.items():
    idx.add_document(id, text)

q = input("Search word: ")
res = idx.search(q)
print(f"Found in: {res}" if res else "Not found.")




#2

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

documents = {
    1: "apple banana orange",
    2: "apple banana",
    3: "banana orange",
    4: "apple"
}

# Boolean Retrieval
def build_index(docs):
    index = {}
    for doc_id, text in docs.items():
        for term in set(text.split()):
            index.setdefault(term, set()).add(doc_id)
    return index

index = build_index(documents)

def boolean_and(terms): return list(set.intersection(*(index.get(t, set()) for t in terms)))
def boolean_or(terms): return list(set.union(*(index.get(t, set()) for t in terms)))
def boolean_not(term): return list(set(documents) - index.get(term, set()))

print("AND:", boolean_and(["apple", "banana"]))
print("OR:", boolean_or(["apple", "orange"]))
print("NOT orange:", boolean_not("orange"))

# Vector Space Model using TF-IDF
texts = list(documents.values())
tfidf = TfidfVectorizer()
vectors = tfidf.fit_transform(texts)
query = tfidf.transform(["apple orange"])

sims = cosine_similarity(query, vectors).flatten()
ranked = sorted(enumerate(sims, start=1), key=lambda x: x[1], reverse=True)

print("\nVector Space Model Ranking (TF-IDF + Cosine):")
for doc_id, score in ranked:
    print(f"Doc {doc_id}: {score:.4f}")




#3

def levenshtein_distance(a, b):
    m, n = len(a)+1, len(b)+1
    dp = [[0]*n for _ in range(m)]
    for i in range(m): dp[i][0] = i
    for j in range(n): dp[0][j] = j
    for i in range(1, m):
        for j in range(1, n):
            cost = 0 if a[i-1] == b[j-1] else 1
            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)
    return dp[-1][-1]

def suggest(word, words):
    return min(words, key=lambda w: levenshtein_distance(word, w))

def retrieve(query, dictionary):
    corrected = ' '.join(suggest(w, dictionary) for w in query.split())
    print(f"Retrieving information for corrected query: '{corrected}'")

input_word = input("Enter a word: ")
dict1 = ["hello", "world", "python", "spell", "correct", "algorithm"]
print(f"Suggested correction for '{input_word}': {suggest(input_word, dict1)}")

user_query = "speling correctin algorithm"
dict2 = ["spelling", "correction", "algorithm", "information", "retrieval", "system"]
retrieve(user_query, dict2)



#4A

def calculate_metrics(retrieved, relevant):
    tp = len(retrieved & relevant)
    fp = len(retrieved - relevant)
    fn = len(relevant - retrieved)
    p = tp / (tp + fp) if (tp + fp) else 0
    r = tp / (tp + fn) if (tp + fn) else 0
    f = 2 * p * r / (p + r) if (p + r) else 0
    return p, r, f

retrieved = {"doc1", "doc2", "doc3", "doc5"}
relevant = {"doc1", "doc4"}
p, r, f = calculate_metrics(retrieved, relevant)
print(f"Precision: {p}\nRecall: {r}\nF-measure: {f}")


#4B

from sklearn.metrics import average_precision_score
y_true = [0, 1, 1, 0, 1, 1]
y_score = [0.1, 0.4, 0.35, 0.8, 0.65, 0.9]
avg_precision = average_precision_score(y_true, y_score)
print(f"Average precision-recall score: {avg_precision}")



#5

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

corpus = [
    "This movie is great and enjoyable.",
    "I really liked this film!",
    "The acting was terrible.",
    "Such a waste of time.",
    "Not worth watching."
]
labels = [1, 1, 0, 0, 0]

X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2, random_state=42)
vec = CountVectorizer()
X_train_vec = vec.fit_transform(X_train)
X_test_vec = vec.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, y_train)
pred = model.predict(X_test_vec)

for t, p in zip(X_test, pred):
    print(f"Test: '{t}' | Predicted: {p}")
print("Accuracy:", accuracy_score(y_test, pred))





#6


import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

docs = [
    "Machine learning is the study of computer algorithms that improve automatically through experience.",
    "Deep learning is a subset of machine learning.",
    "Natural language processing is a field of artificial intelligence.",
    "Computer vision is a field of study that enables computers to interpret and understand the visual world.",
    "Reinforcement learning is a type of machine learning algorithm that teaches an agent how to make decisions in an environment by rewarding desired behaviors.",
    "Information retrieval is the process of obtaining information from a collection of documents.",
    "Text mining is the process of deriving high-quality information from text.",
    "Data clustering is the task of dividing a set of objects into groups.",
    "Hierarchical clustering builds a tree of clusters.",
    "K-means clustering is a method of vector quantization."
]

X = TfidfVectorizer().fit_transform(docs)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X)
print("Silhouette Score:", silhouette_score(X, kmeans.labels_))

for i in range(3):
    idx = np.where(kmeans.labels_ == i)[0]
    print(f"\nCluster {i+1}:")
    for j in idx:
        print("-", docs[j])








#7


import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urlparse

class WebCrawler:
    def __init__(self):
        self.visited_urls = set()

    def crawl(self, url, depth=3, delay=1):
        if depth == 0 or url in self.visited_urls:
            return
        try:
            if not self.is_allowed_by_robots(url):
                return
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                self.index_page(url, soup)
                self.visited_urls.add(url)
                for link in soup.find_all('a'):
                    new_url = link.get('href')
                    if new_url and new_url.startswith('http'):
                        time.sleep(delay)
                        self.crawl(new_url, depth - 1, delay)
        except Exception as e:
            print(f"Error crawling {url}: {e}")

    def is_allowed_by_robots(self, url):
        parsed_url = urlparse(url)
        robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
        try:
            r = requests.get(robots_url)
            if r.status_code == 200 and "Disallow: /" in r.text:
                return False
        except:
            pass
        return True

    def index_page(self, url, soup):
        title = soup.title.string if soup.title else "No title"
        para = soup.find('p').get_text() if soup.find('p') else "No paragraph"
        print(f"Indexing: {url}\nTitle: {title}\nFirst Paragraph: {para}\n")

if __name__ == "__main__":
    crawler = WebCrawler()
    crawler.crawl("https://10fastfingers.com/typing-test/english")







#8


def pagerank(graph, d=0.85, eps=1e-8, max_iter=100):
    n = len(graph)
    pr = {node: 1/n for node in graph}
    for _ in range(max_iter):
        new_pr, diff = {}, 0
        for node in graph:
            rank = (1 - d) / n
            for ref, links in graph.items():
                if node in links:
                    rank += d * pr[ref] / len(links)
            new_pr[node] = rank
            diff = max(diff, abs(rank - pr[node]))
        pr = new_pr
        if diff < eps:
            break
    return pr

web_graph = {'A': ['B', 'C'], 'B': ['C'], 'C': ['A']}
scores = pagerank(web_graph)
for node, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):
    print(f"{node}: {score}")






#9A

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, average_precision_score

def train_ranksvm(X, y):
    model = SVC(kernel='linear')
    model.fit(X, y)
    return model

def predict_ranksvm(model, X):
    return model.decision_function(X)

X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = train_ranksvm(X_train, y_train)
rankings = predict_ranksvm(model, X_test)
y_pred = (rankings > 0).astype(int)
print("Accuracy:", accuracy_score(y_test, y_pred))






#9B

import numpy as np
from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score

X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = SVC(kernel='linear', probability=True)
model.fit(X_train, y_train)
y_pred_prob = model.predict_proba(X_test)[:, 1]
print("Mean Average Precision (MAP):", average_precision_score(y_test, y_pred_prob))






#10A



import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np

nltk.download('punkt')
nltk.download('stopwords')

def sentence_similarity(sent1, sent2):
    stopwords_list = stopwords.words('english')
    word_vector1 = [word.lower() for word in sent1 if word.lower() not in stopwords_list]
    word_vector2 = [word.lower() for word in sent2 if word.lower() not in stopwords_list]
    
    all_words = list(set(word_vector1 + word_vector2))
    vector1 = [word_vector1.count(word) for word in all_words]
    vector2 = [word_vector2.count(word) for word in all_words]
    
    return 1 - cosine_distance(vector1, vector2)

def generate_summary(text, num_sentences):
    sentences = sent_tokenize(text)
    sentence_tokens = [word_tokenize(sentence) for sentence in sentences]
    
    similarity_matrix = np.array([[sentence_similarity(sent1, sent2) for sent2 in sentence_tokens] for sent1 in sentence_tokens])
    scores = np.sum(similarity_matrix, axis=1)
    
    if scores.sum() != 0:
        scores /= scores.sum()
    
    ranked_sentences = [sentence for _, sentence in sorted(zip(scores, sentences), reverse=True)]
    return " ".join(ranked_sentences[:num_sentences])

text = """Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. Text summarization is the process of distilling the most important information to produce a concise and coherent summary. TextRank is an algorithm for extractive summarization."""
summary = generate_summary(text, 2)
print(summary)







#10B



import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

documents = [
    "The cat is on the mat.", 
    "The dog is in the yard.",
    "A bird is flying in the sky.", 
    "The sun is shining brightly."
]

nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('english')

def tokenize(text):
    tokens = nltk.word_tokenize(text)
    return [t.lower() for t in tokens if t.isalnum()]

vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words)
tfidf_matrix = vectorizer.fit_transform(documents)

def answer_question(query, documents, tfidf_matrix, vectorizer):
    query_vec = vectorizer.transform([query])
    sims = cosine_similarity(query_vec, tfidf_matrix)
    return documents[sims.argmax()]

query = "Where is the cat?"
answer = answer_question(query, documents, tfidf_matrix, vectorizer)
print("Answer:", answer)



