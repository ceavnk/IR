#1

import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

nltk.download('stopwords')
nltk.download('punkt')

class InvertedIndex:
    def __init__(self):
        self.index = {}

    def add_document(self, doc_id, content):
        words = word_tokenize(content.lower())
        stop_words = set(stopwords.words('english'))
        for word in set(words):
            if word.isalnum() and word not in stop_words:
                self.index.setdefault(word, []).append(doc_id)

    def search(self, query):
        return self.index.get(query.lower(), [])

# Example
docs = {
    1: "Inverted index is a data structure",
    2: "Example of inverted index construction",
    3: "Algorithm constructs an inverted index"
}

idx = InvertedIndex()
for id, text in docs.items():
    idx.add_document(id, text)

q = input("Search word: ")
res = idx.search(q)
print(f"Found in: {res}" if res else "Not found.")




#2

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

documents = {
    1: "apple banana orange",
    2: "apple banana",
    3: "banana orange",
    4: "apple"
}

# Boolean Retrieval
def build_index(docs):
    index = {}
    for doc_id, text in docs.items():
        for term in set(text.split()):
            index.setdefault(term, set()).add(doc_id)
    return index

index = build_index(documents)

def boolean_and(terms): return list(set.intersection(*(index.get(t, set()) for t in terms)))
def boolean_or(terms): return list(set.union(*(index.get(t, set()) for t in terms)))
def boolean_not(term): return list(set(documents) - index.get(term, set()))

print("AND:", boolean_and(["apple", "banana"]))
print("OR:", boolean_or(["apple", "orange"]))
print("NOT orange:", boolean_not("orange"))

# Vector Space Model using TF-IDF
texts = list(documents.values())
tfidf = TfidfVectorizer()
vectors = tfidf.fit_transform(texts)
query = tfidf.transform(["apple orange"])

sims = cosine_similarity(query, vectors).flatten()
ranked = sorted(enumerate(sims, start=1), key=lambda x: x[1], reverse=True)

print("\nVector Space Model Ranking (TF-IDF + Cosine):")
for doc_id, score in ranked:
    print(f"Doc {doc_id}: {score:.4f}")




#3

def levenshtein_distance(a, b):
    m, n = len(a)+1, len(b)+1
    dp = [[0]*n for _ in range(m)]
    for i in range(m): dp[i][0] = i
    for j in range(n): dp[0][j] = j
    for i in range(1, m):
        for j in range(1, n):
            cost = 0 if a[i-1] == b[j-1] else 1
            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)
    return dp[-1][-1]

def suggest(word, words):
    return min(words, key=lambda w: levenshtein_distance(word, w))

def retrieve(query, dictionary):
    corrected = ' '.join(suggest(w, dictionary) for w in query.split())
    print(f"Retrieving information for corrected query: '{corrected}'")

input_word = input("Enter a word: ")
dict1 = ["hello", "world", "python", "spell", "correct", "algorithm"]
print(f"Suggested correction for '{input_word}': {suggest(input_word, dict1)}")

user_query = "speling correctin algorithm"
dict2 = ["spelling", "correction", "algorithm", "information", "retrieval", "system"]
retrieve(user_query, dict2)



#4A

def calculate_metrics(retrieved, relevant):
    tp = len(retrieved & relevant)
    fp = len(retrieved - relevant)
    fn = len(relevant - retrieved)
    p = tp / (tp + fp) if (tp + fp) else 0
    r = tp / (tp + fn) if (tp + fn) else 0
    f = 2 * p * r / (p + r) if (p + r) else 0
    return p, r, f

retrieved = {"doc1", "doc2", "doc3", "doc5"}
relevant = {"doc1", "doc4"}
p, r, f = calculate_metrics(retrieved, relevant)
print(f"Precision: {p}\nRecall: {r}\nF-measure: {f}")


#4B

from sklearn.metrics import average_precision_score
y_true = [0, 1, 1, 0, 1, 1]
y_score = [0.1, 0.4, 0.35, 0.8, 0.65, 0.9]
avg_precision = average_precision_score(y_true, y_score)
print(f"Average precision-recall score: {avg_precision}")



#5

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score

corpus = [
    "This movie is great and enjoyable.",
    "I really liked this film!",
    "The acting was terrible.",
    "Such a waste of time.",
    "Not worth watching."
]
labels = [1, 1, 0, 0, 0]

X_train, X_test, y_train, y_test = train_test_split(corpus, labels, test_size=0.2, random_state=42)
vec = CountVectorizer()
X_train_vec = vec.fit_transform(X_train)
X_test_vec = vec.transform(X_test)

model = MultinomialNB()
model.fit(X_train_vec, y_train)
pred = model.predict(X_test_vec)

for t, p in zip(X_test, pred):
    print(f"Test: '{t}' | Predicted: {p}")
print("Accuracy:", accuracy_score(y_test, pred))





#6


import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

docs = [
    "Machine learning is the study of computer algorithms that improve automatically through experience.",
    "Deep learning is a subset of machine learning.",
    "Natural language processing is a field of artificial intelligence.",
    "Computer vision is a field of study that enables computers to interpret and understand the visual world.",
    "Reinforcement learning is a type of machine learning algorithm that teaches an agent how to make decisions in an environment by rewarding desired behaviors.",
    "Information retrieval is the process of obtaining information from a collection of documents.",
    "Text mining is the process of deriving high-quality information from text.",
    "Data clustering is the task of dividing a set of objects into groups.",
    "Hierarchical clustering builds a tree of clusters.",
    "K-means clustering is a method of vector quantization."
]

X = TfidfVectorizer().fit_transform(docs)
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10).fit(X)
print("Silhouette Score:", silhouette_score(X, kmeans.labels_))

for i in range(3):
    idx = np.where(kmeans.labels_ == i)[0]
    print(f"\nCluster {i+1}:")
    for j in idx:
        print("-", docs[j])








#7


import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urlparse

class WebCrawler:
    def __init__(self):
        self.visited_urls = set()

    def crawl(self, url, depth=3, delay=1):
        if depth == 0 or url in self.visited_urls:
            return
        try:
            if not self.is_allowed_by_robots(url):
                return
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                self.index_page(url, soup)
                self.visited_urls.add(url)
                for link in soup.find_all('a'):
                    new_url = link.get('href')
                    if new_url and new_url.startswith('http'):
                        time.sleep(delay)
                        self.crawl(new_url, depth - 1, delay)
        except Exception as e:
            print(f"Error crawling {url}: {e}")

    def is_allowed_by_robots(self, url):
        parsed_url = urlparse(url)
        robots_url = f"{parsed_url.scheme}://{parsed_url.netloc}/robots.txt"
        try:
            r = requests.get(robots_url)
            if r.status_code == 200 and "Disallow: /" in r.text:
                return False
        except:
            pass
        return True

    def index_page(self, url, soup):
        title = soup.title.string if soup.title else "No title"
        para = soup.find('p').get_text() if soup.find('p') else "No paragraph"
        print(f"Indexing: {url}\nTitle: {title}\nFirst Paragraph: {para}\n")

if __name__ == "__main__":
    crawler = WebCrawler()
    crawler.crawl("https://10fastfingers.com/typing-test/english")







#8


def pagerank(graph, d=0.85, eps=1e-8, max_iter=100):
    n = len(graph)
    pr = {node: 1/n for node in graph}
    for _ in range(max_iter):
        new_pr, diff = {}, 0
        for node in graph:
            rank = (1 - d) / n
            for ref, links in graph.items():
                if node in links:
                    rank += d * pr[ref] / len(links)
            new_pr[node] = rank
            diff = max(diff, abs(rank - pr[node]))
        pr = new_pr
        if diff < eps:
            break
    return pr

web_graph = {'A': ['B', 'C'], 'B': ['C'], 'C': ['A']}
scores = pagerank(web_graph)
for node, score in sorted(scores.items(), key=lambda x: x[1], reverse=True):
    print(f"{node}: {score}")






#9A

from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, average_precision_score

def train_ranksvm(X, y):
    model = SVC(kernel='linear')
    model.fit(X, y)
    return model

def predict_ranksvm(model, X):
    return model.decision_function(X)

X, y = make_classification(n_samples=100, n_features=20, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

model = train_ranksvm(X_train, y_train)
rankings = predict_ranksvm(model, X_test)
y_pred = (rankings > 0).astype(int)
print("Accuracy:", accuracy_score(y_test, y_pred))






#9B

import numpy as np
from sklearn.svm import SVC
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import average_precision_score

X, y = make_classification(n_samples=1000, n_features=10, n_classes=2, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = SVC(kernel='linear', probability=True)
model.fit(X_train, y_train)
y_pred_prob = model.predict_proba(X_test)[:, 1]
print("Mean Average Precision (MAP):", average_precision_score(y_test, y_pred_prob))






#10A



import nltk
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from nltk.cluster.util import cosine_distance
import numpy as np

nltk.download('punkt')
nltk.download('stopwords')

def sentence_similarity(sent1, sent2):
    stopwords_list = stopwords.words('english')
    word_vector1 = [word.lower() for word in sent1 if word.lower() not in stopwords_list]
    word_vector2 = [word.lower() for word in sent2 if word.lower() not in stopwords_list]
    
    all_words = list(set(word_vector1 + word_vector2))
    vector1 = [word_vector1.count(word) for word in all_words]
    vector2 = [word_vector2.count(word) for word in all_words]
    
    return 1 - cosine_distance(vector1, vector2)

def generate_summary(text, num_sentences):
    sentences = sent_tokenize(text)
    sentence_tokens = [word_tokenize(sentence) for sentence in sentences]
    
    similarity_matrix = np.array([[sentence_similarity(sent1, sent2) for sent2 in sentence_tokens] for sent1 in sentence_tokens])
    scores = np.sum(similarity_matrix, axis=1)
    
    if scores.sum() != 0:
        scores /= scores.sum()
    
    ranked_sentences = [sentence for _, sentence in sorted(zip(scores, sentences), reverse=True)]
    return " ".join(ranked_sentences[:num_sentences])

text = """Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language. Text summarization is the process of distilling the most important information to produce a concise and coherent summary. TextRank is an algorithm for extractive summarization."""
summary = generate_summary(text, 2)
print(summary)







#10B



import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

documents = [
    "The cat is on the mat.", 
    "The dog is in the yard.",
    "A bird is flying in the sky.", 
    "The sun is shining brightly."
]

nltk.download('stopwords')
stop_words = nltk.corpus.stopwords.words('english')

def tokenize(text):
    tokens = nltk.word_tokenize(text)
    return [t.lower() for t in tokens if t.isalnum()]

vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words=stop_words)
tfidf_matrix = vectorizer.fit_transform(documents)

def answer_question(query, documents, tfidf_matrix, vectorizer):
    query_vec = vectorizer.transform([query])
    sims = cosine_similarity(query_vec, tfidf_matrix)
    return documents[sims.argmax()]

query = "Where is the cat?"
answer = answer_question(query, documents, tfidf_matrix, vectorizer)
print("Answer:", answer)





# 1A: Inverted Index Construction and Retrieval System

from collections import defaultdict

# Sample documents
docs = {
    "doc1": "The computer science students are appearing for practical examination.",
    "doc2": "computer science practical examination will start tomorrow."
}

# Preprocessing & index building
def build_inverted_index(docs):
    index = defaultdict(list)
    for doc_id, text in docs.items():
        words = text.lower().split()
        for word in set(words):
            index[word].append(doc_id)
    return index

# Retrieve documents for query terms
def retrieve_documents(index, query_terms):
    result = set(index.get(query_terms[0], []))
    for term in query_terms[1:]:
        result &= set(index.get(term, []))  # AND query
    return result

inverted_index = build_inverted_index(docs)
query = ["computer", "science"]
result = retrieve_documents(inverted_index, query)
print("Inverted Index:", dict(inverted_index))
print("Query Result:", result)




# 1B: Question Answering using Information Extraction

corpus = {
    "doc1": "Tiger is the national animal of India.",
    "doc2": "Peacock is the national bird of India.",
    "doc3": "Mango is the national fruit of India."
}

def answer_question(question, corpus):
    for doc in corpus.values():
        if "national bird" in question.lower() and "national bird" in doc.lower():
            return doc.split("is")[-1].strip().rstrip(".")
    return "Answer not found"

question = "Which is the national bird of India?"
answer = answer_question(question, corpus)
print("Answer:", answer)





# 2A: Spelling Correction using Edit Distance

def edit_distance(str1, str2):
    m, n = len(str1), len(str2)
    dp = [[0]*(n+1) for _ in range(m+1)]
    for i in range(m+1):
        for j in range(n+1):
            if i == 0: dp[i][j] = j
            elif j == 0: dp[i][j] = i
            elif str1[i-1] == str2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
    return dp[m][n]

print("Edit Distance:", edit_distance("nature", "creature"))



# 2B: Boolean Retrieval Model

docs = {
    "doc1": "The cat chased the dog around the garden.",
    "doc2": "She was sitting in the garden last night.",
    "doc3": "I read the book the night before."
}

def boolean_retrieval(docs, query_terms):
    result = []
    for doc_id, text in docs.items():
        text = text.lower()
        if any(term in text for term in query_terms):
            result.append(doc_id)
    return result

query = ["garden", "night"]
print("Matching Docs:", boolean_retrieval(docs, que





# 3A: Web Crawler (Basic)

import requests
from bs4 import BeautifulSoup

def simple_crawler(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    return soup.title.string, soup.get_text()

title, content = simple_crawler("https://example.com")
print("Title:", title)




# 3B: Crawler Challenges (robots.txt, delay)

import time
import urllib.robotparser

def check_robots_txt(base_url, user_agent="*"):
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(base_url + "/robots.txt")
    rp.read()
    return rp.can_fetch(user_agent, base_url)

print("Allowed:", check_robots_txt("https://example.com"))

# Simulating crawl delay
print("Fetching after delay...")
time.sleep(2)  # delay of 2 seconds






# 4A: PageRank Algorithm

def pagerank(graph, d=0.85, iterations=100):
    nodes = graph.keys()
    n = len(nodes)
    ranks = {node: 1/n for node in nodes}

    for _ in range(iterations):
        new_ranks = {}
        for node in nodes:
            incoming = [src for src, dsts in graph.items() if node in dsts]
            rank_sum = sum(ranks[src]/len(graph[src]) for src in incoming)
            new_ranks[node] = (1 - d)/n + d * rank_sum
        ranks = new_ranks
    return ranks

web_graph = {
    "A": ["B", "C", "D"],
    "B": ["C", "E"],
    "C": ["A", "D"],
    "D": [],
    "E": []
}

print("PageRank:", pagerank(web_graph))




# 4B: Extractive Text Summarization

import nltk
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

nltk.download('punkt')
text = """
Natural language processing is a field of computer science, AI and linguistics.
NLP involves natural language understanding and generation.
Text summarization distills important info from source.
"""

sentences = sent_tokenize(text)
cv = CountVectorizer().fit_transform(sentences)
similarity_matrix = (cv * cv.T).toarray()
scores = similarity_matrix.sum(axis=1)
ranked = np.argsort(scores)[::-1]
summary = [sentences[i] for i in ranked[:2]]
print("Summary:", " ".join(summary))





# 5A: Cosine Similarity
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def cosine_similarity_query():
    query = "gold silver truck"
    document = "shipment of gold damaged in a gold fire"
    vectorizer = CountVectorizer().fit_transform([query, document])
    vectors = vectorizer.toarray()
    cos_sim = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
    return cos_sim




# 5B: Question Answering using Information Extraction

corpus = {
    "doc1": "Tiger is the national animal of India.",
    "doc2": "Peacock is the national bird of India.",
    "doc3": "Mango is the national fruit of India."
}

def answer_question(question, corpus):
    for doc in corpus.values():
        if "national bird" in question.lower() and "national bird" in doc.lower():
            return doc.split("is")[-1].strip().rstrip(".")
    return "Answer not found"

question = "Which is the national bird of India?"
answer = answer_question(question, corpus)
print("Answer:", answer)






10
# 6A: Precision, Recall, F1-Score (TP=60, FP=30, FN=20)
def eval_metrics(tp=60, fp=30, fn=20):
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2 * (precision * recall) / (precision + recall)
    return precision, recall, f1



# 6B: Evaluation Toolkit Example
from sklearn.metrics import average_precision_score

def evaluation_toolkit():
    y_true = [0, 1, 1, 0, 1]
    y_scores = [0.1, 0.8, 0.6, 0.3, 0.9]
    avg_prec = average_precision_score(y_true, y_scores)
    return avg_prec





9
# 7A: Vector Space Model with TF-IDF
from sklearn.feature_extraction.text import TfidfVectorizer

def tfidf_vector_space():
    documents = [
        "Document about python programming language and data analysis.",
        "Document discussing machine learning algorithms and programming techniques.",
        "Overview of natural language processing and its applications."
    ]
    query = ["python programming"]
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(documents + query)
    return cosine_similarity(vectors[-1], vectors[:-1]).flatten()




# 7B:Cosine Similarity with TF-IDF Vectors

from sklearn.metrics.pairwise import cosine_similarity

# Last vector is query, rest are documents
cosine_similarities = cosine_similarity(vectors[-1], vectors[:-1]).flatten()

# Print similarity scores
for i, score in enumerate(cosine_similarities):
    print(f"Cosine similarity between Query and Document {i+1}: {score:.4f}")






16
# 8A: K-means Clustering on Documents
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

def document_clustering():
    docs = [
        "Machine learning is the study of computer algorithms that improve through experience.",
        "Deep learning is a subset of machine learning.",
        "Natural language processing is a field of artificial intelligence.",
        "Computer vision enables computers to interpret the visual world.",
        "Reinforcement learning is a machine learning algorithm.",
        "Information retrieval is the process of obtaining information from a collection.",
        "Text mining derives high-quality information from text.",
        "Data clustering divides a set of objects into groups.",
        "Hierarchical clustering builds a tree of clusters.",
        "K-means clustering is a method of vector quantization."
    ]
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(docs)
    model = KMeans(n_clusters=3, random_state=42)
    labels = model.fit_predict(X)
    return labels





# 8B: Classification Evaluation
from sklearn.metrics import precision_score, recall_score, f1_score

def classification_metrics():
    y_true = [0, 1, 1, 0, 1]
    y_pred = [0, 1, 1, 0, 1]
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    return precision, recall, f1




2
#9:Implement an inverted index for:

Document1: "The quick brown fox jumped over the lazy dog"

Document2: "The lazy dog slept in the sun"

Find documents with terms "lazy sun".

docs = {
    "Doc1": "The quick brown fox jumped over the lazy dog",
    "Doc2": "The lazy dog slept in the sun"
}

# Build inverted index
index = {}
for doc, text in docs.items():
    for word in text.lower().split():
        index.setdefault(word, set()).add(doc)

# Search for "lazy sun"
result = index.get("lazy", set()) & index.get("sun", set())
print("Matching docs:", result)





#8B:Boolean retrieval model:

# Documents
docs = {
    "D1": "BSc lectures start at 7.",
    "D2": "My lectures are over.",
    "D3": "Today is a holiday."
}

# Normalize and tokenize
inverted_index = {}
for doc_id, text in docs.items():
    for word in text.lower().strip('.').split():
        inverted_index.setdefault(word, set()).add(doc_id)

# Boolean NOT query
all_docs = set(docs.keys())
not_lectures = all_docs - inverted_index.get("lectures", set())

print("Matching documents:", not_lectures)


#11:Classification using Naïve Bayes on 20_newsgroups (Positive vs Negative Sentiment)

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# Load 2 categories to simulate binary sentiment
categories = ['rec.sport.baseball', 'talk.politics.misc']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

# Vectorize text
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(data.data)
y = data.target  # 0 = baseball (positive), 1 = politics (negative)

# Train Naïve Bayes classifier
model = MultinomialNB()
model.fit(X, y)

# Evaluate
y_pred = model.predict(X)
print("Naïve Bayes Classification Report:")
print(classification_report(y, y_pred, target_names=data.target_names))





#11B:Classification using SVM on 20_newsgroups (Multiclass: e.g., alt.atheism, etc.)

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report

# Load multiple categories
categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

# SVM pipeline: TF-IDF + SVC
model = make_pipeline(TfidfVectorizer(stop_words='english'), SVC(kernel='linear'))
model.fit(data.data, data.target)

# Predict & evaluate
y_pred = model.predict(data.data)
print("SVM Classification Report:")
print(classification_report(data.target, y_pred, target_names=data.target_names))





#12: Question Answering System using TF-IDF and Cosine Similarity

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 📚 Corpus
corpus = [
    "India has the second-largest population in the world.",
    "It is surrounded by oceans from three sides which are Bay Of Bengal in the east, the Arabian Sea in the west and Indian oceans in the south.",
    "Tiger is the national animal of India.",
    "Peacock is the national bird of India.",
    "Mango is the national fruit of India."
]

# ❓ Query
query = "Which is the national bird of India?"

# Combine corpus and query for vectorization
documents = corpus + [query]

# 🔤 TF-IDF Vectorization
vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

# 📐 Compute cosine similarity between query and all corpus sentences
cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()

# 🔍 Find best match
best_match_index = cosine_similarities.argmax()
best_answer = corpus[best_match_index]

print("Query:", query)
print("Answer:", best_answer)





#13:pageRank algorithm
import numpy as np

# 📄 Pages: A, B, C, D
pages = ['A', 'B', 'C', 'D']
n = len(pages)

# 🔗 Link Structure: Define who links to whom
links = {
    'A': ['B', 'C'],
    'B': ['C', 'D'],
    'C': ['A', 'D'],
    'D': ['B']
}

# Build the link matrix M (n x n)
M = np.zeros((n, n))

for i, page in enumerate(pages):
    targets = links[page]
    if targets:
        for target in targets:
            j = pages.index(target)
            M[j, i] = 1 / len(targets)  # Distribute rank evenly

# 🔁 PageRank iteration
d = 0.85  # damping factor
r = np.ones(n) / n  # initial rank (uniform distribution)

# Iterate until convergence (100 iterations in this case)
for _ in range(100):
    r_new = (1 - d) / n + d * np.dot(M, r)
    
    # Check for convergence (stop if the rank vector does not change much)
    if np.linalg.norm(r_new - r, 1) < 1e-6:
        break
    r = r_new

# 📊 Output final PageRank scores
for i, score in enumerate(r):
    print(f"Page {pages[i]}: {score:.4f}")






# ==================== 17A: Inverted Index Construction ====================
from collections import defaultdict

docs = {
    "doc1": "our class meeting starts soon",
    "doc2": "my class starts at 6"
}

inverted_index = defaultdict(list)

for doc_id, text in docs.items():
    for word in text.lower().split():
        if doc_id not in inverted_index[word]:
            inverted_index[word].append(doc_id)

print("Inverted Index:")
for term, doc_list in inverted_index.items():
    print(f"{term}: {doc_list}")

# ==================== 17B: Document Retrieval for "class meeting" ====================
query_terms = ["class", "meeting"]
result_docs = set(docs.keys())

for term in query_terms:
    result_docs &= set(inverted_index.get(term, []))

print("\nDocuments containing 'class' and 'meeting':", result_docs)

# ==================== 18A: TF-IDF Vector Space Model ====================
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "The sun is the star at the center of the solar system.",
    "She wore a beautiful dress to the party last night.",
    "The book on the table caught my attention immediately."
]
query = ["solar system"]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus + query)

print("\nTF-IDF Matrix:")
print(X.toarray())

# ==================== 18B: Clustering with KMeans ====================
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X[:-1])  # exclude query

print("\nCluster Labels for Documents:")
print(kmeans.labels_)

# ==================== 20A: QA System – "Which is the national bird of India?" ====================
corpus_qa = {
    "1": "India has the second-largest population in the world.",
    "2": "Tiger is the national animal of India.",
    "3": "Peacock is the national bird of India.",
    "4": "Mango is the national fruit of India."
}

question = "Which is the national bird of India?"

# Simple QA using keyword match
for doc_id, text in corpus_qa.items():
    if "national bird" in text.lower():
        print("\nAnswer to QA Query:")
        print(text)
        break

# ==================== 20B: Text Summarization (Extractive) ====================
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

text = """
Natural language processing (NLP) is a field of computer science, artificial intelligence,
and computational linguistics concerned with the interactions between computers and human languages.
Many challenges in NLP involve natural language understanding, generation, and machine learning.
Text summarization is the process of distilling the most important information from a source.
"""

sentences = [s.strip() for s in text.strip().split('\n') if s]
vectorizer = CountVectorizer().fit_transform(sentences)
vectors = vectorizer.toarray()
sentence_scores = np.sum(vectors, axis=1)

top_sentence_index = np.argmax(sentence_scores)
print("\nSummary (Extractive):")
print(sentences[top_sentence_index])





# 1A: Inverted Index Construction and Retrieval System

from collections import defaultdict

# Sample documents
from collections import defaultdict  # ✅ Required import

docs = {
    "doc1": "The computer science students are appearing for practical examination.",
    "doc2": "computer science practical examination will start tomorrow."
}

# Preprocessing & index building
def build_inverted_index(docs):
    index = defaultdict(list)
    for doc_id, text in docs.items():
        words = text.lower().split()
        for word in set(words):
            index[word].append(doc_id)
    return index

# Retrieve documents for query terms
def retrieve_documents(index, query_terms):
    result = set(index.get(query_terms[0], []))
    for term in query_terms[1:]:
        result &= set(index.get(term, []))  # AND query
    return result

inverted_index = build_inverted_index(docs)
query = ["computer", "science"]
result = retrieve_documents(inverted_index, query)
print("Inverted Index:", dict(inverted_index))
print("Query Result:", result)




# 1B: Question Answering using Information Extraction

corpus = {
    "doc1": "Tiger is the national animal of India.",
    "doc2": "Peacock is the national bird of India.",
    "doc3": "Mango is the national fruit of India."
}

def answer_question(question, corpus):
    for doc in corpus.values():
        if "national bird" in question.lower() and "national bird" in doc.lower():
            return doc.split("is")[-1].strip().rstrip(".")
    return "Answer not found"

question = "Which is the national bird of India?"
answer = answer_question(question, corpus)
print("Answer:", answer)





# 2A: Spelling Correction using Edit Distance

def edit_distance(str1, str2):
    m, n = len(str1), len(str2)
    dp = [[0]*(n+1) for _ in range(m+1)]
    for i in range(m+1):
        for j in range(n+1):
            if i == 0: dp[i][j] = j
            elif j == 0: dp[i][j] = i
            elif str1[i-1] == str2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
    return dp[m][n]

print("Edit Distance:", edit_distance("nature", "creature"))



# 2B: Boolean Retrieval Model

docs = {
    "doc1": "The cat chased the dog around the garden.",
    "doc2": "She was sitting in the garden last night.",
    "doc3": "I read the book the night before."
}

def boolean_retrieval(docs, query_terms):
    result = []
    for doc_id, text in docs.items():
        text = text.lower()
        if any(term in text for term in query_terms):
            result.append(doc_id)
    return result

query = ["garden", "night"]
print("Matching Docs:", boolean_retrieval(docs, query))





# 3A: Web Crawler (Basic)

import requests
from bs4 import BeautifulSoup

def simple_crawler(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.text, "html.parser")
    return soup.title.string if soup.title else "No Title", soup.get_text()

title, content = simple_crawler("https://example.com")
print("Title:", title)




# 3B: Crawler Challenges (robots.txt, delay)

import time
import urllib.robotparser

def check_robots_txt(base_url, user_agent="*"):
    rp = urllib.robotparser.RobotFileParser()
    rp.set_url(base_url + "/robots.txt")
    rp.read()
    return rp.can_fetch(user_agent, base_url)

print("Allowed:", check_robots_txt("https://example.com"))

# Simulating crawl delay
print("Fetching after delay...")
time.sleep(2)  # delay of 2 seconds






# 4A: PageRank Algorithm

def pagerank(graph, d=0.85, iterations=100):
    nodes = graph.keys()
    n = len(nodes)
    ranks = {node: 1/n for node in nodes}

    for _ in range(iterations):
        new_ranks = {}
        for node in nodes:
            incoming = [src for src, dsts in graph.items() if node in dsts]
            rank_sum = sum(ranks[src]/len(graph[src]) for src in incoming)
            new_ranks[node] = (1 - d)/n + d * rank_sum
        ranks = new_ranks
    return ranks

web_graph = {
    "A": ["B", "C", "D"],
    "B": ["C", "E"],
    "C": ["A", "D"],
    "D": [],
    "E": []
}

print("PageRank:", pagerank(web_graph))




# 4B: Extractive Text Summarization

import nltk
from nltk.tokenize import sent_tokenize
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

nltk.download('punkt')
text = """
Natural language processing is a field of computer science, AI and linguistics.
NLP involves natural language understanding and generation.
Text summarization distills important info from source.
"""

sentences = sent_tokenize(text)
cv = CountVectorizer().fit_transform(sentences)
similarity_matrix = (cv * cv.T).toarray()
scores = similarity_matrix.sum(axis=1)
ranked = np.argsort(scores)[::-1]
summary = [sentences[i] for i in ranked[:2]]
print("Summary:", " ".join(summary))





# 5A: Cosine Similarity
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def cosine_similarity_query():
    query = "gold silver truck"
    document = "shipment of gold damaged in a gold fire"
    vectorizer = CountVectorizer().fit_transform([query, document])
    vectors = vectorizer.toarray()
    cos_sim = cosine_similarity([vectors[0]], [vectors[1]])[0][0]
    return cos_sim

# Call the function and print the result
similarity_score = cosine_similarity_query()
print("Cosine Similarity:", similarity_score).





# 5B: Question Answering using Information Extraction

corpus = {
    "doc1": "Tiger is the national animal of India.",
    "doc2": "Peacock is the national bird of India.",
    "doc3": "Mango is the national fruit of India."
}

def answer_question(question, corpus):
    for doc in corpus.values():
        if "national bird" in question.lower() and "national bird" in doc.lower():
            return doc.split("is")[-1].strip().rstrip(".")
    return "Answer not found"

question = "Which is the national bird of India?"
answer = answer_question(question, corpus)
print("Answer:", answer)






10
# 6A: Precision, Recall, F1-Score (TP=60, FP=30, FN=20)
def eval_metrics(tp=60, fp=30, fn=20):
    precision = tp / (tp + fp)
    recall = tp / (tp + fn)
    f1 = 2 * (precision * recall) / (precision + recall)
    return precision, recall, f1

precision, recall, f1 = eval_metrics()
print(f"Precision: {precision:.2f}, Recall: {recall:.2f}, F1-Score: {f1:.2f}")



# 6B: Evaluation Toolkit Example
from sklearn.metrics import average_precision_score

def evaluation_toolkit():
    y_true = [0, 1, 1, 0, 1]
    y_scores = [0.1, 0.8, 0.6, 0.3, 0.9]
    avg_prec = average_precision_score(y_true, y_scores)
    return avg_prec

print(f"Average Precision Score: {evaluation_toolkit():.2f}")







9
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def tfidf_vector_space():
    documents = [
        "Document about python programming language and data analysis.",
        "Document discussing machine learning algorithms and programming techniques.",
        "Overview of natural language processing and its applications."
    ]
    query = ["python programming"]
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(documents + query)
    return cosine_similarity(vectors[-1], vectors[:-1]).flatten()

# Call and print the result
similarities = tfidf_vector_space()
for i, score in enumerate(similarities):
    print(f"Cosine similarity between Query and Document {i+1}: {score:.4f}")





# 7B:Cosine Similarity with TF-IDF Vectors
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

def tfidf_vector_space():
    documents = [
        "Document about python programming language and data analysis.",
        "Document discussing machine learning algorithms and programming techniques.",
        "Overview of natural language processing and its applications."
    ]
    query = ["python programming"]
    vectorizer = TfidfVectorizer()
    vectors = vectorizer.fit_transform(documents + query)
    return vectors

# Call the function and get the vectors
vectors = tfidf_vector_space()

# Compute cosine similarities (last vector is the query, others are documents)
cosine_similarities = cosine_similarity(vectors[-1], vectors[:-1]).flatten()

# Print similarity scores
for i, score in enumerate(cosine_similarities):
    print(f"Cosine similarity between Query and Document {i+1}: {score:.4f}")






16
# 8A: K-means Clustering on Documentsfrom sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

def document_clustering():
    docs = [
        "Machine learning is the study of computer algorithms that improve through experience.",
        "Deep learning is a subset of machine learning.",
        "Natural language processing is a field of artificial intelligence.",
        "Computer vision enables computers to interpret the visual world.",
        "Reinforcement learning is a machine learning algorithm.",
        "Information retrieval is the process of obtaining information from a collection.",
        "Text mining derives high-quality information from text.",
        "Data clustering divides a set of objects into groups.",
        "Hierarchical clustering builds a tree of clusters.",
        "K-means clustering is a method of vector quantization."
    ]
    vectorizer = TfidfVectorizer()
    X = vectorizer.fit_transform(docs)
    model = KMeans(n_clusters=3, random_state=42)
    labels = model.fit_predict(X)
    return labels

# Call the function and print the result
labels = document_clustering()
print("Cluster Labels:", labels)






# 8B: Classification Evaluation
from sklearn.metrics import precision_score, recall_score, f1_score

def classification_metrics():
    y_true = [0, 1, 1, 0, 1]
    y_pred = [0, 1, 1, 0, 1]
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    return precision, recall, f1

# Call the function and print the result
precision, recall, f1 = classification_metrics()
print(f"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}")





2
#9:Implement an inverted index for:

docs = {
    "Doc1": "The quick brown fox jumped over the lazy dog",
    "Doc2": "The lazy dog slept in the sun"
}

# Build inverted index
index = {}
for doc, text in docs.items():
    for word in text.lower().split():
        index.setdefault(word, set()).add(doc)

# Search for terms "lazy" and "sun"
search_terms = ["lazy", "sun"]
matching_docs = set(docs.keys())  # Start with all docs

# Intersect docs for each search term
for term in search_terms:
    matching_docs &= index.get(term, set())

print("Matching docs:", matching_docs)





#8B:Boolean retrieval model:

# Documents
docs = {
    "D1": "BSc lectures start at 7.",
    "D2": "My lectures are over.",
    "D3": "Today is a holiday."
}

# Normalize and tokenize
inverted_index = {}
for doc_id, text in docs.items():
    for word in text.lower().strip('.').split():
        inverted_index.setdefault(word, set()).add(doc_id)

# Boolean NOT query
all_docs = set(docs.keys())
not_lectures = all_docs - inverted_index.get("lectures", set())

print("Matching documents:", not_lectures)


#11:Classification using Naïve Bayes on 20_newsgroups (Positive vs Negative Sentiment)

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report

# Load 2 categories to simulate binary sentiment
categories = ['rec.sport.baseball', 'talk.politics.misc']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

# Vectorize text
vectorizer = CountVectorizer(stop_words='english')
X = vectorizer.fit_transform(data.data)
y = data.target  # 0 = baseball (positive), 1 = politics (negative)

# Train Naïve Bayes classifier
model = MultinomialNB()
model.fit(X, y)

# Evaluate
y_pred = model.predict(X)
print("Naïve Bayes Classification Report:")
print(classification_report(y, y_pred, target_names=data.target_names))





#11B:Classification using SVM on 20_newsgroups (Multiclass: e.g., alt.atheism, etc.)

from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.pipeline import make_pipeline
from sklearn.metrics import classification_report

# Load multiple categories
categories = ['alt.atheism', 'comp.graphics', 'sci.space', 'talk.religion.misc']
data = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))

# SVM pipeline: TF-IDF + SVC
model = make_pipeline(TfidfVectorizer(stop_words='english'), SVC(kernel='linear'))
model.fit(data.data, data.target)

# Predict & evaluate
y_pred = model.predict(data.data)
print("SVM Classification Report:")
print(classification_report(data.target, y_pred, target_names=data.target_names))





#12: Question Answering System using TF-IDF and Cosine Similarity
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

corpus = [
    "India has the second-largest population in the world.",
    "Tiger is the national animal of India.",
    "Peacock is the national bird of India.",
    "Mango is the national fruit of India."
]

query = "Which is the national bird of India?"

documents = corpus + [query]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(documents)

cosine_similarities = cosine_similarity(tfidf_matrix[-1], tfidf_matrix[:-1]).flatten()

best_match_index = cosine_similarities.argmax()
best_answer = corpus[best_match_index]

print("Query:", query)
print("Answer:", best_answer)



#13:pageRank algorithm
import numpy as np

# 📄 Pages: A, B, C, D
pages = ['A', 'B', 'C', 'D']
n = len(pages)

# 🔗 Link Structure: Define who links to whom
links = {
    'A': ['B', 'C'],
    'B': ['C', 'D'],
    'C': ['A', 'D'],
    'D': ['B']
}

# Build the link matrix M (n x n)
M = np.zeros((n, n))

for i, page in enumerate(pages):
    targets = links[page]
    if targets:
        for target in targets:
            j = pages.index(target)
            M[j, i] = 1 / len(targets)  # Distribute rank evenly

# 🔁 PageRank iteration
d = 0.85  # damping factor
r = np.ones(n) / n  # initial rank (uniform distribution)

# Iterate until convergence (100 iterations in this case)
for _ in range(100):
    r_new = (1 - d) / n + d * np.dot(M, r)
    
    # Check for convergence (stop if the rank vector does not change much)
    if np.linalg.norm(r_new - r, 1) < 1e-6:
        break
    r = r_new

# 📊 Output final PageRank scores
for i, score in enumerate(r):
    print(f"Page {pages[i]}: {score:.4f}")




# ==================== 17B: Document Retrieval for "class meeting" ====================
docs = {
    "Doc1": "The class meeting was held on Monday.",
    "Doc2": "A group of students met after the class.",
    "Doc3": "Meeting for the upcoming event will be scheduled next week."
}

query_terms = ["class", "meeting"]
result_docs = set(docs.keys())

for term in query_terms:
    result_docs &= {doc for doc, text in docs.items() if term in text.lower()}

print("\nDocuments containing 'class' and 'meeting':", result_docs)

# ==================== 18A: TF-IDF Vector Space Model ====================
from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
    "The sun is the star at the center of the solar system.",
    "She wore a beautiful dress to the party last night.",
    "The book on the table caught my attention immediately."
]
query = ["solar system"]

vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus + query)

print("\nTF-IDF Matrix:")
print(X.toarray())

# ==================== 18B: Clustering with KMeans ====================
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X[:-1])  # exclude query

print("\nCluster Labels for Documents:")
print(kmeans.labels_)

# ==================== 20A: QA System – "Which is the national bird of India?" ====================
corpus_qa = {
    "1": "India has the second-largest population in the world.",
    "2": "Tiger is the national animal of India.",
    "3": "Peacock is the national bird of India.",
    "4": "Mango is the national fruit of India."
}

question = "Which is the national bird of India?"

# Simple QA using keyword match
for doc_id, text in corpus_qa.items():
    if "national bird" in text.lower():
        print("\nAnswer to QA Query:")
        print(text)
        break

# ==================== 20B: Text Summarization (Extractive) ====================
from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

text = """
Natural language processing (NLP) is a field of computer science, artificial intelligence,
and computational linguistics concerned with the interactions between computers and human languages.
Many challenges in NLP involve natural language understanding, generation, and machine learning.
Text summarization is the process of distilling the most important information from a source.
"""

sentences = [s.strip() for s in text.strip().split('\n') if s]
vectorizer = CountVectorizer().fit_transform(sentences)
vectors = vectorizer.toarray()
sentence_scores = np.sum(vectors, axis=1)

top_sentence_index = np.argmax(sentence_scores)
print("\nSummary (Extractive):")
print(sentences[top_sentence_index])


